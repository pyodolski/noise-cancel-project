import torch
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim
from models.dfnet_model import SimpleDFNet
from datasets.df_dataset import DFDataset

print("ğŸš€ í›ˆë ¨ ì‹œì‘ ì¤€ë¹„ ì¤‘...")
# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
BATCH_SIZE = 4
EPOCHS = 30
LEARNING_RATE = 0.001
SAMPLE_RATE = 16000

# collate_fn: ê¸¸ì´ ì•ˆ ë§ëŠ” ì˜¤ë””ì˜¤ë“¤ì„ ìë¥¸ ë’¤ batchë¡œ ë¬¶ê¸°
def collate_fn(batch):
    min_len = min(min(len(noisy), len(clean)) for noisy, clean in batch)
    noisy_batch = torch.stack([noisy[:min_len] for noisy, _ in batch])
    clean_batch = torch.stack([clean[:min_len] for _, clean in batch])
    return noisy_batch, clean_batch

# ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA ìš°ì„ )
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ë°ì´í„°ì…‹ ë° DataLoader ì •ì˜
dataset = DFDataset(
    noisy_dir='data/mixtures_train',
    clean_dir='data/clean_train',
    sample_rate=SAMPLE_RATE
)
dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn  # ğŸ”¥ ê¸¸ì´ ë¬¸ì œ í•´ê²°
)

# ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ ì„¸íŒ…
model = SimpleDFNet().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# í•™ìŠµ ë£¨í”„


for epoch in range(EPOCHS):
    print(f"ğŸ“¦ Epoch {epoch + 1} ì‹œì‘")
    model.train()
    total_loss = 0.0

    for noisy_batch, clean_batch in dataloader:
        print("ğŸ‘‰ ë°°ì¹˜ ë¡œë“œë¨")
        noisy_batch = noisy_batch.to(device)
        clean_batch = clean_batch.to(device)

        # ìˆœì „íŒŒ
        output = model(noisy_batch)

        # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ
        loss = criterion(output, clean_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    print(f"[Epoch {epoch+1}/{EPOCHS}] Loss: {avg_loss:.4f}")

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), "trained_dfnet.pt")
print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: trained_dfnet.pt")
